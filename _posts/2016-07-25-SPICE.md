---
title: "SPICE: Semantic Propositional Image Caption Evaluation"
excerpt: "Evaluating image captions using scene graph tuples."
layout: project
tags: [ECCV, European Conference on Computer Vision, 2016, image captioning, evaluation metric, Microsoft COCO]
permalink: /spice/
header:
  teaser: spice-concept-small.png
toc:
  title: "SPICE"
  entries:
  - "Abstract"
  - "ECCV 2016 Paper"
  - "Code"
  - "Examples"
  - "Microsoft COCO Results"

---

[Peter Anderson](/), [Basura Fernando](http://users.cecs.anu.edu.au/~basura/), [Mark Johnson](http://web.science.mq.edu.au/~mjohnson/), [Stephen Gould](http://users.cecs.anu.edu.au/~sgould/)


### Abstract
There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. In this paper we hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined *SPICE*. Evaluations indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as *which caption-generator best understands colors?* and *can caption-generators count?*

<figure class="align-center"> 
  <img src="{{ site.url }}{{ site.baseurl }}/images/spice-concept.png" alt="">
  <figcaption>Reference and candidate captions are mapped through dependency parse trees (top) to semantic scene graphs (right) - encoding the objects (red), attributes (green), and relations (blue) present. Caption quality is determined using an F-score calculated over tuples in the candidate and reference scene graphs</figcaption>
</figure>

### ECCV 2016 Paper

[SPICE: Semantic Propositional Image Caption Evaluation.](/images/SPICE.pdf) Peter Anderson, Basura Fernando, Mark Johnson and Stephen Gould. In *Proceedings of the European Conference on Computer Vision (ECCV), Amsterdam, the Netherlands, October 2016*.
{: .notice--info}

If reporting SPICE scores, please reference the SPICE paper:

```
@inproceedings{spice2016,
  title     = {SPICE: Semantic Propositional Image Caption Evaluation},
  author    = {Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},
  year      = {2016},
  booktitle = {ECCV}
}
```


### Code

SPICE can be downloaded via the link below. This will download a 31 MB zip file containing (1) the SPICE code jar, (2) the libraries required to run SPICE (except for Stanford CoreNLP) and (3) documentation / source code for the project. Unzip this file, download Stanford CoreNLP using the included download script and youâ€™re ready to use it.

[Download SPICE-1.0.zip](/images/SPICE-1.0.zip){: .btn .btn--info}

Alternatively, a fork of the Microsoft COCO caption evaluation code including SPICE is available on Github. 
SPICE source is also on Github.

[MS COCO evaluation code on Github](https://github.com/peteanderson80/coco-caption){: .btn .btn--success} 
[SPICE source on Github](https://github.com/peteanderson80/SPICE){: .btn .btn--inverse}


### Examples

*Coming soon!*

### Microsoft COCO Results

*Coming soon!*

